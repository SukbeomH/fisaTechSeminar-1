- 1. Vector 검색이란?

  - 1.1 텍스트 기반 검색과 차이점
  - 1.2 BOW에서 Embedding으로

- 2. 검색을 위한 유사성 측정 기법

  - 2.1 거리를 측정하는 계산식
    - 2.1.1 코사인 유사성 (Cosine Similarity)
    - 2.1.2 유클리드 거리 (Euclidean Distance)
    - 2.1.3 점 곱 유사성 (Dot Product Similarity)
  - 2.2 ElasticSearch에서의 유사성 측정 적용

- 3. Approximate Nearest Neighbor [공식문서](https://www.elastic.co/kr/blog/understanding-ann)

  - 3.1 ANN 검색의 필요성과 중요성
  - 3.2 ANN 검색 구현 방법
    - 3.2.1 지역성 기반 해싱(LSH)
    - 3.2.2 KD 트리
    - 3.2.3 Annoy
    - 3.2.4 선형 스캔 알고리즘

---

---

#### **1. Vector 검색이란?**

##### **1.1 텍스트 기반 검색과 차이점**

- **텍스트 기반 검색의 개념**:

  - 전통적인 텍스트 기반 검색은 주로 키워드 매칭에 의존합니다. 사용자가 입력한 쿼리와 일치하는 키워드나 구문을 문서에서 찾아내고, 해당 문서를 검색 결과로 반환합니다.
  - **한계점**: 이 방식은 텍스트의 문맥이나 의미를 충분히 반영하지 못하기 때문에, 문서의 실제 내용과 사용자의 의도 간의 간극이 발생할 수 있습니다.

- **벡터 기반 검색의 필요성**:
  - 벡터 기반 검색은 텍스트나 이미지를 고차원 벡터로 변환하여 의미적 유사성을 평가하는 방식입니다. 이를 통해 문맥과 의미를 더 잘 반영한 검색이 가능하며, 자연어 처리(NLP), 이미지 검색, 추천 시스템 등 다양한 응용 분야에서 활용됩니다.
  - **응용 분야**: 예를 들어, 사용자가 입력한 문장의 의미를 벡터로 변환하고, 이 벡터와 유사한 벡터를 가진 문서를 검색하는 방식이 가능합니다.

##### **1.2 BOW에서 Embedding으로**

- **BOW (Bag of Words) 모델**:

  - BOW는 텍스트를 단어의 출현 빈도로 표현하는 방법입니다. 각 문서는 단어의 빈도 벡터로 변환되며, 이 벡터를 사용해 문서 간 유사성을 계산합니다.
  - **한계**: BOW 모델은 단어의 순서나 문맥을 반영하지 않기 때문에, 의미적 유사성을 제대로 포착하지 못할 수 있습니다.

- **Embedding으로의 전환**:
  - **Word2Vec, BERT** 등의 임베딩 기법은 단어와 문장을 고차원 벡터 공간에 매핑하여 의미를 반영한 벡터를 생성합니다.
  - **Embedding의 장점**: 임베딩 모델은 문맥과 의미를 반영하기 때문에, 단순한 단어 빈도 이상으로 텍스트 간의 의미적 유사성을 더 정확하게 측정할 수 있습니다.
  - **BOW와 임베딩 비교**: BOW는 단순하고 직관적이지만, 의미적 유사성 측면에서 임베딩 모델이 더 강력합니다.

---

#### **2. 검색을 위한 유사성 측정 기법**

##### **2.1 거리를 측정하는 계산식**

###### **2.1.1 코사인 유사성 (Cosine Similarity)**

- **정의**: 코사인 유사성은 두 벡터 간의 각도를 측정하여 유사성을 평가합니다. 두 벡터가 이루는 각도의 코사인을 계산하여 유사성을 나타내며, 코사인 값이 1에 가까울수록 두 벡터가 유사합니다.
- **공식**:
  \[
  \text{Cosine Similarity} = \frac{A \cdot B}{\|A\| \|B\|}
  \]
  여기서 \(A\)와 \(B\)는 비교하려는 두 벡터입니다.
- **예시**: 텍스트 임베딩에서 두 문장의 의미적 유사성을 비교하는 데 자주 사용됩니다.

###### **2.1.2 유클리드 거리 (Euclidean Distance)**

- **정의**: 유클리드 거리는 두 벡터 간의 직선 거리를 측정하는 방식입니다. 벡터 간의 거리가 짧을수록 유사성이 크다고 판단합니다.
- **공식**:
  \[
  \text{Euclidean Distance} = \sqrt{\sum\_{i=1}^{n} (A_i - B_i)^2}
  \]
  여기서 \(A_i\)와 \(B_i\)는 각각 벡터 \(A\)와 \(B\)의 i번째 차원의 값입니다.
- **예시**: 이미지 검색에서 피처 벡터 간의 시각적 유사성을 평가하는 데 사용됩니다.

###### **2.1.3 점 곱 유사성 (Dot Product Similarity)**

- **정의**: 점 곱 유사성은 두 벡터의 크기와 방향에 따라 유사성을 측정합니다. 값이 클수록 유사성이 높다고 판단됩니다.
- **공식**:
  \[
  \text{Dot Product} = A \cdot B = \sum\_{i=1}^{n} A_i \times B_i
  \]
  여기서 \(A_i\)와 \(B_i\)는 각각 벡터 \(A\)와 \(B\)의 i번째 차원의 값입니다.
- **예시**: 추천 시스템에서 사용자가 선호하는 항목과 유사한 항목을 찾는 데 자주 사용됩니다.

##### **2.2 ElasticSearch에서의 유사성 측정 적용**

- **ElasticSearch의 스크립트 함수**:
  - ElasticSearch는 `cosineSimilarity`, `euclideanDistance`, `dotProduct` 등의 스크립트 함수를 제공하여 벡터 간의 유사성을 계산할 수 있습니다.
- **실제 적용 방법**:
  - 사용자는 ElasticSearch 쿼리에서 이러한 함수를 활용하여, 벡터 데이터 간의 유사성을 기반으로 검색 결과를 필터링하고 정렬할 수 있습니다.
  - **예시**: 자연어 질문을 벡터로 변환한 후, 이 벡터와 유사한 벡터를 가진 문서를 ElasticSearch에서 검색하는 방법입니다.

---

#### **3. Approximate Nearest Neighbor (ANN) 검색**

##### **3.1 ANN 검색의 필요성과 중요성**

- **고차원 벡터 공간의 문제점**:
  - 고차원 벡터 공간에서는 모든 데이터 포인트 간의 거리를 계산하는 것이 비효율적입니다. 이는 연산 시간이 오래 걸리고, 대규모 데이터셋에서는 현실적으로 불가능할 수 있습니다.
- **ANN 검색의 이점**:
  - 근사 최근접 이웃(ANN) 검색은 정확성을 일부 포기하는 대신, 빠르고 효율적으로 유사한 데이터를 찾는 방법입니다. 이는 실시간 검색이 중요한 애플리케이션에서 특히 유용합니다.
- **ElasticSearch의 ANN 지원**:
  - ElasticSearch는 ANN 검색을 지원하여 대규모 데이터셋에서도 고성능의 벡터 검색을 가능하게 합니다. 이를 통해 고차원 벡터 데이터를 효율적으로 관리하고 활용할 수 있습니다.

##### **3.2 ANN 검색 구현 방법**

###### **3.2.1 지역성 기반 해싱 (LSH)**

- **LSH의 개념**:
  - Locality-Sensitive Hashing(LSH)은 유사한 벡터들이 동일한 해시 버킷에 매핑되도록 설계된 해싱 기법입니다. 이를 통해 고차원 데이터에서도 빠르게 유사한 벡터를 찾을 수 있습니다.
- **적용 사례**:
  - LSH는 고차원 데이터에서의 검색 속도를 높이는 데 유리하며, 주로 텍스트 임베딩과 이미지 피처 벡터 검색에서 사용됩니다.

###### **3.2.2 KD 트리**

- **KD 트리의 개념**:
  - KD 트리는 각 차원별로 데이터를 분할하여 트리 구조로 저장하는 방법입니다. 주로 저차원 데이터에서 효율적이며, 트리를 탐색하여 근사 최근접 이웃을 찾습니다.
- **제한 사항**:
  - KD 트리는 차원이 증가할수록 성능이 급격히 저하되기 때문에, 고차원 데이터에는 적합하지 않을 수 있습니다.

###### **3.2.3 Annoy**

- **Annoy의 개념**:
  - Annoy는 여러 개의 랜덤 KD 트리를 생성하여, 근사 최근접 이웃을 효율적으로 찾는 방법입니다. 트리 탐색 결과를 결합하여 유사한 데이터를 빠르게 검색할 수 있습니다.
- **응용 분야**:
  - Annoy는 고차원 데이터에서의 검색이 필요한 추천 시스템, 음악 검색 등에서 널리 사용됩니다.

###### **3.2.4 선형 스캔 알고리즘**

- **선형 스캔의 개념**:
  - 선형 스캔 알고리즘은 데이터셋 내의 모든 데이터 포인트를 일일이 비교하여 유사성을 평가하는 방식입니다. 이는 정확성이 높지만, 대규모 데이터셋에서는 비효율적일 수 있습니다.
- **적용 시 고려사항**:

  - 선형 스캔은 작은 데이터셋이나 정확성이 중요한 경우에 적합하지만, 데이터셋이 커질수록 계산 비용이 급증합니다. 따라서 ANN 기법과 병행하여 사용할 수 있습니다.

- **[ElasticSearch 공식 문서](https://www.elastic.co/kr/blog/understanding-ann) 참고**:
  - ElasticSearch에서 ANN 검색을 이해하고 구현하는 데 도움이 되는 공식 문서입니다. ANN 검색의 개념, 구현 방법, 실제 적용 사례 등이 자세히 설명되어 있습니다.

---
