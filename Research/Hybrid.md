[Original TEXT](https://opster.com/guides/elasticsearch/machine-learning/elasticsearch-hybrid-search/)

---

하이브리드 검색 쿼리의 어휘 검색과 의미 검색 결과를 병합하기 위해서는 검색된 문서들의 상대적 관련성을 유지하면서 두 결과 세트를 결합해야 하는데, 이는 해결하기 복잡한 문제입니다. 다행히도 이를 해결하기 위해 사용할 수 있는 몇 가지 기존 방법이 있으며, 그 중 두 가지 매우 일반적인 방법은 Convex Combination(CC)과 Reciprocal Rank Fusion(RRF)입니다.

기본적으로 Convex Combination(또는 Linear Combination이라고도 함)은 어휘 검색 결과와 의미 검색 결과의 정규화된 점수를 각각의 가중치와 함께 결합하는 것을 목표로 합니다. 이때 가중치는 0과 1 사이의 값을 가지며, 이를 통해:

CC는 어휘 점수와 의미 점수의 가중 평균으로 볼 수 있지만, OpenSearch와는 달리 가중치가 퍼센트로 표현되지 않으며, 즉 100%로 합산될 필요가 없습니다. 0과 1 사이의 가중치는 관련 쿼리를 디부스트(deboost)하는 데 사용되며, 1보다 큰 가중치는 이를 부스트(boost)하는 데 사용됩니다.

반면, RRF는 점수 보정이나 정규화를 필요로 하지 않으며, 다음과 같은 공식을 사용하여 결과 세트 내에서 문서의 순위에 따라 문서를 단순히 점수화합니다. 여기서 k는 낮은 순위의 문서의 중요성을 조정하기 위한 임의의 상수입니다:

CC와 RRF 모두 각각의 장단점이 있으며, 아래의 표 1에서 강조된 바와 같이:

| Convex Combination                                     | Reciprocal Rank Fusion                                   |
| ------------------------------------------------------ | -------------------------------------------------------- |
| **장점**                                               | **장점**                                                 |
| - 가중치의 좋은 보정이 RRF보다 CC를 더 효과적으로 만듦 | - 보정이 필요 없음                                       |
| - 완전히 비지도 학습                                   | - 최소/최대 점수를 알 필요 없음                          |
| **단점**                                               | **단점**                                                 |
| - 가중치의 좋은 보정을 요구함                          | - k 값의 튜닝이 간단하지 않음                            |
| - 최적의 가중치는 데이터 세트마다 다름                 | - 결과 세트 크기 증가 시 순위 품질이 영향을 받을 수 있음 |

이러한 장단점에 대해서는 가정과 테스트된 데이터 세트에 따라 의견이 다를 수 있음을 유의해야 합니다. 간단히 요약하면, RRF는 CC보다 약간 덜 정확한 점수를 제공하지만, "플러그 앤 플레이" 방식으로 사용할 수 있으며, 라벨이 있는 쿼리 세트로 가중치를 미세 조정할 필요가 없다는 큰 장점이 있습니다.

버전 2.10에서 하이브리드 CC 방식을 채택한 OpenSearch와 달리, Elastic은 CC와 RRF 접근 방식을 모두 지원하기로 결정했습니다. 이 기사의 후반부에서 이를 어떻게 구현했는지 살펴볼 것입니다. 해당 선택의 논리적 배경에 대해 더 알고 싶다면 Elastic 블로그의 이 훌륭한 기사와 Elastician Philipp Krenn이 Haystack 2023에서 발표한 RRF에 대한 이 훌륭한 강연을 확인해 보십시오.

---

### Elasticsearch에서 하이브리드 검색의 해부

이전 기사에서 간단히 언급했듯이, Elasticsearch에서의 벡터 검색 지원은 `dense_vector` 필드 타입을 통해 이루어졌습니다. 이는 주로 0이 아닌 값을 포함하고, 비정형 데이터를 다차원 공간에서 의미를 표현하는 벡터를 생성하는 밀집 벡터 모델(dense vector model)을 활용한 것입니다.

그러나 밀집 모델이 의미 검색을 수행하는 유일한 방법은 아닙니다. Elasticsearch는 희소 벡터 모델(sparse vector model)을 사용하는 대안도 제공합니다. Elastic은 특정 도메인에 맞게 훈련되지 않은(out-of-domain) 희소 벡터 모델인 Elastic Learned Sparse EncodeR(약칭 ELSER)을 개발했습니다. 이 모델은 약 30,000개의 용어로 구성된 어휘에 대해 사전 훈련되었으며, 희소 모델이기 때문에 대부분의 벡터 값(즉, 99.9% 이상)은 0입니다.

ELSER 모델이 작동하는 방식은 매우 간단합니다. 인덱싱 시점에는 용어/가중치 쌍을 포함하는 희소 벡터가 `inference` 인제스트 프로세서를 통해 생성되고, `rank_features` 타입의 필드에 저장됩니다. 이는 `dense_vector` 필드 타입의 희소 대응물입니다. 쿼리 시점에는 `text_expansion`이라는 특정 DSL 쿼리가 원래 쿼리 용어를 ELSER 모델 어휘에 있는 가장 유사한 용어들로 대체합니다.

#### 희소 모델과 밀집 모델 중 어느 것을 선택할까?

하이브리드 검색 쿼리로 넘어가기 전에, 희소 모델과 밀집 모델 간의 차이점을 간단히 강조하고자 합니다. 아래의 그림 2는 "the quick brown fox"라는 텍스트 조각이 각 모델에 의해 어떻게 인코딩되는지를 보여줍니다.

희소 모델의 경우, 원래의 네 가지 용어는 그와 가깝거나 먼 관계에 있는 30개의 가중치가 부여된 용어들로 확장됩니다. 확장된 용어의 가중치가 높을수록 원래 용어와의 관련성이 더 큽니다. ELSER 어휘가 30,000개 이상의 용어를 포함하기 때문에, "the quick brown fox"를 나타내는 벡터는 그만큼의 차원을 가지며, 비제로(non-zero) 값은 약 0.1%만 포함됩니다(즉, ~30 / 30,000). 이 때문에 이러한 모델을 희소(sparse) 모델이라고 부릅니다.

---

[VectorSearch](https://opster.com/guides/opensearch/opensearch-machine-learning/introduction-to-vector-search/)

---

### 벡터 검색에 대한 간략한 소개

**작성자: Opster 전문가 팀 - Valentin Crettaz**

**업데이트: 2024년 3월 10일 | 12분 소요**

**간단한 링크:**

- 개요
- 벡터는 새로운 것이 아니다
- 벡터 검색 vs. 어휘 검색
- 임베딩 벡터
- 비법
- 거리와 유사성
- L1 거리
- L2 거리
- Linf 거리
- 코사인 유사성
- 내적 유사성
- 빠른 요약
- 벡터 검색 알고리즘 및 기술
- 선형 검색
- K-차원 트리
- 역파일 인덱스
- 양자화
- 계층적 탐색 가능한 작은 세계(HNSW)
- 지역 민감 해싱(LSH)
- 결론을 내리자

### 개요

이 기사는 벡터 검색(또는 의미 검색)의 복잡성을 탐구하고, OpenSearch와 Elasticsearch에서 어떻게 구현되는지를 설명하는 5부작 시리즈의 첫 번째 부분입니다.

이 첫 번째 부분에서는 임베딩 벡터의 기본 개념과 벡터 검색이 내부적으로 어떻게 작동하는지를 일반적으로 소개하는 데 중점을 둡니다. 이 기사의 내용은 더 이론적이며 OpenSearch에 국한되지 않기 때문에, 동일한 벡터 검색 엔진인 Apache Lucene을 기반으로 하는 Elasticsearch에도 유효합니다.

첫 번째 기사에서 배운 모든 지식을 바탕으로, 두 번째 부분에서는 OpenSearch에서 k-NN 플러그인 또는 2.9 버전에서 일반적으로 제공된 새로운 신경망 검색 플러그인을 사용하여 벡터 검색을 설정하는 방법을 안내합니다.

세 번째 부분에서는 OpenSearch 하이브리드 검색에 대해 다루며, 앞서 배운 내용을 바탕으로 OpenSearch에서 강력한 하이브리드 검색 쿼리를 작성하는 방법을 심화 학습합니다.

네 번째 부분에서는 Elasticsearch에서 벡터 검색을 설정하는 방법을 다루고, 다섯 번째 부분에서는 Elasticsearch 하이브리드 검색을 다루며, 이는 두 번째와 세 번째 부분과 유사하되 Elasticsearch에 중점을 둡니다.

이 기사의 실제 내용에 들어가기 전에, 벡터의 역사를 잠시 돌아보고 의미 검색에서 벡터가 왜 중요한 개념인지 살펴보겠습니다.

### 벡터는 새로운 것이 아니다

아마도 2022년 11월 ChatGPT가 등장한 이후로 "벡터 검색"에 대해 듣지 않거나 읽지 않는 날이 없다는 것에 모두 동의할 것입니다. 벡터 검색은 어디에나 있으며, 너무나도 보편적이어서 이 기술이 방금 나온 최첨단 기술처럼 느껴지지만, 실제로 이 기술은 60년 이상 존재해왔습니다!

이 주제에 대한 연구는 1960년대 중반에 시작되었으며, 1978년에 정보 검색 전문가 Gerard Salton과 그의 Cornell University 동료들이 첫 번째 연구 논문을 발표했습니다. Salton의 밀집 및 희소 벡터 모델에 대한 연구는 현대 벡터 검색 기술의 뿌리를 형성합니다.

지난 20년 동안 그의 연구를 바탕으로 많은 벡터 DBMS가 만들어졌고, 시장에 출시되었습니다. 여기에는 Elasticsearch와 이를 포크한 OpenSearch 프로젝트가 포함되며, 둘 다 2019년에 벡터 검색 작업을 시작한 Apache Lucene 프로젝트를 기반으로 합니다.

벡터는 이제 어디에나 존재하며, 그 이론과 내부 작동 방식을 이해하는 것이 중요합니다. 이를 살펴보기 전에, 벡터 검색과 어휘 검색의 차이를 간략히 검토하여 그들이 어떻게 다르며 어떻게 상호 보완할 수 있는지 이해해 봅시다.

### 벡터 검색 vs. 어휘 검색

벡터 검색을 소개하는 쉬운 방법은 아마도 여러분이 익숙한 기존의 어휘 검색과 비교하는 것입니다. 벡터 검색은 의미 검색으로도 알려져 있으며, 어휘 검색과 매우 다르게 작동합니다. 어휘 검색은 OpenSearch와 Elasticsearch에서 오랫동안 사용해온 검색 방식입니다.

간단히 말해, 어휘 검색은 색인되고 쿼리된 내용의 실제 의미를 이해하려고 하지 않으며, 대신 사용자가 쿼리에서 입력한 단어의 문자 그대로 또는 변형(어간 추출, 동의어 등)과 데이터베이스에 이전에 색인된 모든 문자를 유사성 알고리즘(TF-IDF 및 BM25 등)을 사용하여 일치시키는 데 주력합니다. 아래 그림 1은 어휘 검색이 어떻게 작동하는지를 보여주는 간단한 예시입니다:

**그림 1: 어휘 검색의 간단한 예**

왼쪽 상단의 세 문서는 토큰화되고 분석됩니다. 그런 다음, 결과 용어들이 역인덱스에 색인되며, 이는 단순히 분석된 용어들을 해당 용어를 포함하는 문서 ID에 매핑합니다. 모든 용어는 단 한 번만 나타나며, 어떤 문서도 공유되지 않음을 주목하십시오. "nice german teacher"를 검색하면 세 문서가 모두 다른 점수를 가지고 일치하지만, 실제로는 쿼리의 진정한 의미를 포착하지 못합니다.

아래 그림 2에서 볼 수 있듯이, 다의어 또는 동형어(예: right, palm, bat, mean 등)를 처리할 때는 더욱 까다로워집니다. 예를 들어, "right"라는 단어는 세 가지 다른 의미를 가질 수 있으며, 이 경우 어떻게 작동하는지 살펴봅시다.

**그림 2: 동형어 검색**

"나는 옳지 않다"라는 문구를 검색하면 첫 번째로 반환된 결과와 정확히 반대 의미를 가진 문서가 반환됩니다. 동일한 용어를 다르게 배열하여 다른 의미를 가지도록 쿼리하면(예: "turn right"와 "right turn"), 동일한 결과(즉, 세 번째 문서 "Take a right turn")가 반환됩니다. 우리의 쿼리는 지나치게 단순화되어 있으며 구문 일치와 같은 고급 쿼리를 사용하지 않지만, 이는 어휘 검색이 색인된 내용과 검색된 내용의 실제 의미를 이해하지 못한다는 것을 보여줍니다. 명확하지 않다면 걱정하지 마세요. 세 번째 기사에서 이 예시를 다시 살펴보아 벡터 검색이 어떻게 도움이 될 수 있는지 알아보겠습니다.

어휘 검색에 대한 공정한 평가를 위해, 구조화된 데이터를 색인하는 방법(예: 매핑, 텍스트 분석, 인제스트 파이프라인 등)과 쿼리를 작성하는 방법(예: 스마트하게 작성된 DSL 쿼리, 쿼리 용어 분석 등)을 제어할 수 있다면, 어휘 검색 엔진으로 놀라운 성과를 낼 수 있습니다. OpenSearch와 Elasticsearch의 어휘 검색 기능에 대한 기록은 정말 놀랍습니다. 그들이 달성한 것과 지난 몇 년 동안 어휘 검색 분야를 얼마나 대중화하고 발전시켰는지는 정말 놀라운 일입니다.

그러나 비정형 데이터를 쿼리하는 지원을 제공해야 하는 경우(예: 이미지, 비디오, 오디오, 원시 텍스트 등) 사용자가 자유 텍스트 질문을 할 수 있게 해야 한다면 어휘 검색은 한계에 부딪힙니다. 또한 쿼리가 텍스트가 아닐 수도 있으며, 이미지일 수도 있습니다. 이처럼 비정형 데이터를 다룰 때 어휘 검색이 적절하지 않은 주요 이유는 비정형 데이터는 구조화된 데이터와 같은 방식으로 색인되거나 쿼리될 수 없기 때문입니다. 비정형 데이터를 다룰 때 의미론이 중요해집니다. 의미론이란 무엇일까요? 간단히 말해, 의미입니다!

예를 들어, 이미지 검색 엔진(Google 이미지 검색 또는 렌즈 등)을 생각해봅시다. 이미지를 드래그 앤 드롭하면, Google 의미 검색 엔진이 쿼리한 이미지와 가장 유사한 이미지를 찾아 반환합니다. 아래 그림 3에서 왼쪽에 있는 독일 셰퍼드의 사진과 오른쪽에 검색된 유사한 사진들이 있으며, 첫 번째 결과는 제공된 사진과 동일한 사진(즉, 가장 유사한 사진)입니다.

**그림 3: 이미지 검색**

비록 인간에게는 간단하고 논리적으로 보이지만, 컴퓨터에게는 완전히 다른 이야기입니다. 벡터 검색이 이러한 작업을 가능하게 하고 달성하는 데 도움을 줍니다. 벡터 검색이 발휘하는 힘은 최근에 세계가 목격한 것처럼 엄청납니다. 이제 본격적으로 그 내부를 살펴봅시다.

### 임베딩 벡터

앞서 보았듯이, 어휘 검색 엔진에서는 텍스트와 같은 구조화된 데이터를 용어로 쉽게 토큰화하여 검색 시 일치시킬 수 있지만, 비정형 데이터는

이미지, 비디오, 오디오 등의 큰 이진 객체와 같은 다양한 형태를 취할 수 있으며, 동일한 토큰화 과정에 적합하지 않습니다. 또한 의미 검색의 전체 목적은 데이터를 의미에 따라 검색할 수 있도록 색인하는 것입니다. 이를 어떻게 달성할까요? 답은 두 단어로 요약됩니다: 기계 학습! 또는 더 정확하게 말하면 딥러닝!

딥러닝은 다층 신경망 기반의 모델을 사용하는 기계 학습의 특정 영역으로, 데이터를 점진적으로 실제 의미로 추출할 수 있습니다. 이러한 신경망 모델의 작동 방식은 인간의 뇌에서 영감을 받았습니다. 아래 그림 4는 입력 및 출력 계층뿐만 아니라 여러 숨겨진 계층이 있는 신경망의 모습을 보여줍니다:

**그림 4: 신경망 계층**

신경망 계층
진정한 신경망의 성과는 하나의 비정형 데이터를 임베딩 벡터 또는 단순히 임베딩이라고 알려진 부동 소수점 값의 연속으로 변환할 수 있다는 점입니다. 인간으로서 우리는 벡터를 2차원 또는 3차원 공간에서 시각화하면 잘 이해할 수 있습니다. 벡터의 각 구성 요소는 2D x-y 평면 또는 3D x-y-z 공간에서 좌표를 나타냅니다.

그러나 신경망 모델이 작동하는 임베딩 벡터는 수백 또는 수천 개의 차원을 가질 수 있으며 단순히 다차원 공간에서 하나의 점을 나타냅니다. 벡터의 각 차원은 비정형 데이터의 특징이나 특성을 나타냅니다. 이 개념을 2048차원의 임베딩 벡터로 이미지를 변환하는 딥러닝 모델로 설명해봅시다. 이 모델은 우리가 그림 3에서 사용한 독일 셰퍼드 사진을 아래 표에 표시된 임베딩 벡터로 변환합니다. 첫 번째 및 마지막 세 요소만 표시되어 있으며, 표에는 2042개의 열/차원이 더 존재합니다.

| is_red | is_dog | blue_sky | …   | no_grass | german_shepherd | is_tree |
| ------ | ------ | -------- | --- | -------- | --------------- | ------- |
| 0.0121 | 0.9572 | 0.8735   | …   | 0.1198   | 0.9712          | 0.0512  |

각 열은 모델의 차원을 나타내며, 기본 신경망이 모델링하려고 하는 특징 또는 특성을 나타냅니다. 모델에 주어진 각 입력은 이 2048차원 각각과 얼마나 유사한지에 따라 특성화됩니다. 따라서 임베딩 벡터의 각 요소 값은 해당 입력이 특정 차원과 얼마나 유사한지를 나타냅니다. 이 예시에서 모델은 개와 독일 셰퍼드 간의 높은 유사성과 약간의 파란 하늘의 존재를 감지했습니다.

어휘 검색에서 용어가 일치하거나 일치하지 않을 수 있는 반면, 벡터 검색에서는 비정형 데이터 조각이 모델이 지원하는 각 차원과 얼마나 유사한지에 대한 훨씬 더 나은 감각을 얻을 수 있습니다. 따라서 임베딩 벡터는 비정형 데이터의 의미론적 표현으로서 훌륭한 역할을 합니다.

### 비법

이제 비정형 데이터가 딥러닝 신경망에 의해 잘게 잘라져 수많은 차원을 따라 데이터의 유사성을 포착하는 임베딩 벡터로 변환되는 방법을 알았으므로, 이 벡터의 일치 방법을 이해해야 합니다. 그 답은 매우 간단합니다. 서로 가까운 임베딩 벡터는 의미적으로 유사한 데이터를 나타냅니다. 따라서 벡터 데이터베이스를 쿼리할 때 검색 입력(이미지, 텍스트 등)은 먼저 모든 비정형 데이터를 색인하는 데 사용된 동일한 모델을 사용하여 임베딩 벡터로 변환되며, 궁극적인 목표는 쿼리 벡터와 가장 가까운 인접 벡터를 찾는 것입니다. 즉, 쿼리 벡터와 데이터베이스에 색인된 모든 벡터 간의 "거리" 또는 "유사성"을 측정하는 방법을 알아내는 것이 전부입니다.

### 거리와 유사성

다행히도 벡터 산술 덕분에 두 벡터 간의 거리를 측정하는 것은 간단한 문제입니다. 이제 OpenSearch 및 Elasticsearch와 같은 최신 벡터 검색 데이터베이스에서 지원하는 가장 인기 있는 거리 및 유사성 함수를 살펴보겠습니다. 이제 약간의 수학이 나옵니다!

#### L1 거리

L1 거리, 또는 맨해튼 거리라고도 불리는 이 거리는 두 벡터 x와 y의 각 요소 간의 절대 차이의 합으로 측정됩니다. 당연히 거리 d가 작을수록 두 벡터는 더 가깝습니다. 공식은 아래와 같이 매우 간단합니다:

**L1 거리 공식**

시각적으로, L1 거리는 아래 그림 4와 같이 나타낼 수 있습니다:

**그림 4: 두 벡터 간의 L1 거리 시각화**

두 벡터 간의 L1 거리 시각화
예를 들어, 두 벡터 x = (1, 2)와 y = (4, 3)를 사용하면, L1 거리는 | 1 – 4 | + | 2 – 3 | = 4가 됩니다.

#### L2 거리

L2 거리, 또는 유클리드 거리라고도 불리는 이 거리는 두 벡터 x와 y의 각 요소 간의 차이의 제곱합을 구한 다음, 그 결과의 제곱근을 취하여 측정됩니다. 이는 두 점 사이의 가장 짧은 경로입니다. L1과 유사하게, 거리 d가 작을수록 두 벡터는 더 가깝습니다:

**L2 거리 공식**

L2 거리는 아래 그림 5에 나타나 있습니다:

**그림 5: 두 벡터 간의 L2 거리 시각화**

두 벡터 간의 L2 거리 시각화
L1 거리에 사용된 동일한 두 샘플 벡터 x와 y를 다시 사용하여, L2 거리를 (1 – 4)² + (2 – 3)² = 10으로 계산할 수 있습니다. 10의 제곱근을 취하면 3.16이 됩니다.

#### Linf 거리

Linf(무한 L) 거리, 또는 체비쇼프 거리(체스보드 거리)라고도 불리는 이 거리는 단순히 두 요소 간의 가장 긴 거리 또는 축/차원 중 하나를 따라 측정된 가장 긴 거리로 정의됩니다. 공식은 매우 간단하며 아래에 표시됩니다:

**Linf 거리 공식**

Linf 거리는 아래 그림 6에 나타나 있습니다:

**그림 6: 두 벡터 간의 Linf 거리 시각화**

두 벡터 간의 Linf 거리 시각화
다시 동일한 샘플 벡터 x와 y를 사용하여, Linf 거리를 max( | 1 – 4 | , | 2 – 3 | ) = max(3, 1) = 3으로 계산할 수 있습니다.

#### 코사인 유사성

L1, L2 및 Linf와 달리, 코사인 유사성은 두 벡터 x와 y 간의 거리를 측정하는 것이 아니라, 그들의 상대적 각도를 측정합니다. 즉, 두 벡터가 대략 같은 방향을 가리키고 있는지를 측정합니다. 유사성 s가 클수록 두 벡터는 더 "가깝습니다". 공식은 아래와 같이 매우 간단합니다:

**코사인 유사성 공식**

두 벡터 간의 코사인 유사성을 나타내는 방법은 아래 그림 7에 나와 있습니다:

**그림 7: 두 벡터 간의 코사인 유사성 시각화**

두 벡터 간의 코사인 유사성 시각화
게다가, 코사인 값은 항상 [-1, 1] 구간에 있기 때문에, -1은 반대 유사성(즉, 두 벡터 간의 180° 각도), 0은 관련 없는 유사성(즉, 90° 각도), 1은 동일(즉, 0° 각도)을 의미합니다. 이는 아래 그림 8에 나와 있습니다:

**그림 8: 코사인 유사성 스펙트럼**

코사인 유사성 스펙트럼
다시 한 번, 동일한 샘플 벡터 x와 y를 사용하여 위의 공식을 사용하여 코사인 유사성을 계산해보겠습니다. 먼저, 두 벡터의 내적을 (1・4) + (2・3) = 10으로 계산할 수 있습니다. 그런 다음, 두 벡터의 길이(크기라고도 함)를 곱합니다: (1² + 2²)¹/₂ + (4² + 3²

)¹/₂ = 11.18034. 마지막으로, 내적을 곱한 길이로 나누어 10 / 11.18034 = 0.894427(즉, 26° 각도)로 계산하며, 이는 1에 매우 가까우므로 두 벡터는 상당히 유사하다고 할 수 있습니다.

#### 내적 유사성

코사인 유사성의 단점 중 하나는 두 벡터 간의 각도만 고려하고 크기(길이)는 고려하지 않는다는 점입니다. 이는 두 벡터가 대략 같은 방향을 가리키지만, 하나는 다른 하나보다 훨씬 길면 두 벡터가 여전히 유사한 것으로 간주된다는 것을 의미합니다. 내적 유사성은 벡터의 각도와 크기를 모두 고려하여 이 문제를 개선하며, 훨씬 더 정확한 유사성 지표를 제공합니다.

내적 유사성을 계산하는 데 사용되는 공식은 두 가지가 있으며, 첫 번째 공식은 이전에 보았던 코사인 유사성의 분자와 동일합니다:

**내적 유사성 공식 1**

두 번째 공식은 단순히 두 벡터의 길이를 그들 사이의 각도의 코사인으로 곱한 것입니다:

**내적 유사성 공식 2**

내적 유사성은 아래 그림 9에 시각화되어 있습니다:

**그림 9: 두 벡터 간의 내적 유사성 시각화**

마지막으로, 샘플 벡터 x와 y를 사용하여 첫 번째 공식을 사용하여 내적 유사성을 계산합니다. 이는 코사인 유사성에서 했던 것처럼 (1・4) + (2・3) = 10입니다.

두 번째 공식을 사용하면 두 벡터의 길이를 곱합니다: (1² + 2²)¹/₂ + (4² + 3²)¹/₂ = 11.18034, 그리고 두 벡터 간의 26° 각도의 코사인을 곱하면 11.18034・cos(26°) = 10이 됩니다.

여기서 주목할 만한 점은 모든 벡터가 먼저 정규화된 경우(즉, 길이가 1인 경우), 내적 유사성은 코사인 유사성과 정확히 동일해진다는 것입니다(왜냐하면 |x| |y| = 1이기 때문입니다). 나중에 보겠지만, 벡터를 정규화하는 것은 유사성이 단순히 각도에 초점을 맞출 수 있도록 벡터의 크기를 무시하는 좋은 관행입니다. 이는 또한 색인 및 쿼리 시점에서 거리 계산 속도를 높여주며, 이는 수십억 개의 벡터를 처리할 때 큰 문제로 발전할 수 있습니다.

### 빠른 요약

와우, 지금까지 정말 많은 정보를 다루었습니다. 잠시 멈추고 우리가 어디에 있는지 간단히 요약해봅시다. 우리는 다음을 배웠습니다…

- 의미 검색은 비정형 데이터를 다차원 임베딩 벡터로 변환하는 데 뛰어난 딥러닝 신경망 모델에 기반합니다.
- 모델의 각 차원은 비정형 데이터의 특징 또는 특성을 나타냅니다.
- 임베딩 벡터는 주어진 비정형 데이터 조각이 각 차원과 얼마나 유사한지를 나타내는 유사성 값의 연속입니다.
- 두 벡터가 "가까울수록"(즉, 가장 가까운 이웃일수록), 의미적으로 유사한 개념을 더 많이 나타냅니다.
- 거리 함수(L1, L2, Linf)는 두 벡터가 얼마나 가까운지를 측정할 수 있게 해줍니다.
- 유사성 함수(코사인 및 내적)는 두 벡터가 얼마나 같은 방향을 향하고 있는지를 측정할 수 있게 해줍니다.

이제 마지막으로 남은 부분은 벡터 검색 엔진 자체입니다. 쿼리가 들어오면, 쿼리는 먼저 벡터화되고, 그런 다음 벡터 검색 엔진은 해당 쿼리 벡터와 가장 가까운 인접 벡터를 찾습니다. 데이터베이스의 모든 벡터와 쿼리 벡터 간의 거리나 유사성을 측정하는 브루트포스 접근 방식은 작은 데이터 세트에서는 작동할 수 있지만, 벡터 수가 증가함에 따라 빠르게 한계를 드러냅니다. 즉, 어떻게 수백만, 수십억, 심지어 수조 개의 벡터를 색인하고, 쿼리 벡터의 가장 가까운 이웃을 합리적인 시간 안에 찾을 수 있을까요? 여기서 우리는 더 스마트해지고, 벡터를 색인하는 최적의 방법을 찾아야 합니다. 이를 통해 정확도를 크게 저하시키지 않고도 가장 가까운 이웃을 최대한 빨리 찾아낼 수 있습니다.

### 벡터 검색 알고리즘 및 기술

수년 동안, 다양한 연구팀들이 매우 독창적인 벡터 검색 알고리즘을 개발하는 데 많은 노력을 기울여 왔습니다. 여기서는 주요 알고리즘을 간략히 소개하겠습니다. 사용 사례에 따라 어떤 것이 더 적합할 수 있습니다.

#### 선형 검색

앞서 데이터베이스에 있는 모든 벡터와 쿼리 벡터를 비교하는 브루트포스 접근 방식을 언급했을 때 선형 검색 또는 평면 색인에 대해 간략히 언급했습니다. 이는 작은 데이터 세트에서는 잘 작동할 수 있지만, 벡터와 차원의 수가 증가함에 따라 성능이 급격히 감소합니다(O(n) 복잡성).

다행히도, 임베딩 벡터 간의 거리를 사전에 계산하고 유사한 벡터를 클러스터, 트리, 해시 또는 그래프를 사용하여 가까이 유지하는 근사 최근접 이웃(ANN)이라는 더 효율적인 접근 방식이 있습니다. 이러한 접근 방식은 일반적으로 100% 정확도를 보장하지 않기 때문에 "근사"라고 불립니다. 궁극적인 목표는 가능한 한 빨리 검색 범위를 줄여 유사한 벡터를 포함할 가능성이 가장 높은 영역에만 집중하거나, 벡터의 차원 수를 줄이는 것입니다.

#### K-차원 트리

K-차원 트리 또는 KD 트리는 k차원 공간에서 포인트를 저장하고 벡터가 색인되는 작은 좌우 트리로 검색 공간을 계속해서 이분화하는 이진 검색 트리의 일반화입니다. 검색 시점에 알고리즘은 쿼리 벡터(그림 10의 빨간 점) 주변의 몇 가지 트리 분기만 방문하여 가장 가까운 이웃(그림 10의 녹색 점)을 찾으면 됩니다. k개 이상의 이웃이 요청된 경우 알고리즘이 더 많은 이웃을 찾을 때까지 노란 영역이 확장됩니다.

**그림 10: KD 트리 알고리즘**

KD 트리 알고리즘
KD 트리 알고리즘의 가장 큰 장점은 로컬화된 일부 트리 분기에만 집중할 수 있으므로 대부분의 벡터를 고려 대상에서 제거할 수 있다는 것입니다. 그러나 이 알고리즘의 효율성은 차원 수가 증가함에 따라 감소합니다. 이는 더 낮은 차원 공간보다 더 많은 분기를 방문해야 하기 때문입니다.

#### 역파일 인덱스

역파일 인덱스(IVF) 접근 방식은 벡터를 가까운 중심에 할당하는 공간 분할 알고리즘입니다. 2차원 공간에서 이는 아래 그림 11에 나와 있는 보로노이 다이어그램으로 가장 잘 시각화됩니다:

**그림 11: 2차원 공간에서의 역파일 인덱스의 보로노이 표현**

2차원 공간에서의 역파일 인덱스의 보로노이 표현
위의 2차원 공간이 20개의 클러스터로 분할되어 있으며, 각 클러스터의 중심은 검은 점으로 표시됩니다. 공간에 있는 모든 임베딩 벡터는 자신과 가장 가까운 클러스터의 중심에 할당됩니다. 검색 시점에 알고리즘은 먼저 쿼리 벡터와 가장 가까운 중심을 찾아 해당 영역에 집중할 수 있으며, 필요한 경우 주변 영역도 마찬가지로 집중할 수 있습니다.

이 알고리즘은 높은 차원 공간에서 사용할 때 KD 트리와 동일한 문제를 겪습니다. 이는 차원의 저주라고 하며, 공간의 부피가 너무 커져 모든 데이터가 희박해지고 더 정확한 결과를 얻기 위해 필요한 데이터 양이 기하급수적으로 증가할 때 발생합니다. 데이터가 희박해지면 이러한 공간 분할 알고리즘이 데이터를 클러스터로 구성하는 것이 더 어려워집니다. 다행히도, 아래에서 설명하는 것처럼 이 문제를 완화하는 다른 알고리즘과 기술이 있습니다.

#### 양자화

양자화는 임베딩 벡터의 정밀도를 낮추어 데이터베이스의 전체 크기를 줄일 수 있는 압축 기반 접근 방식입니다. 이는 스칼라 양자화(SQ)를

사용하여 부동 소수점 벡터 값을 정수 값으로 변환함으로써 달성할 수 있습니다. 이는 데이터베이스 크기를 8배 줄일 뿐만 아니라 메모리 소비를 줄이고 검색 시점에서 벡터 간의 거리 계산 속도를 높입니다.

또 다른 기술로는 제품 양자화(PQ)가 있으며, 먼저 공간을 더 낮은 차원 하위 공간으로 나눈 다음, 클러스터링 알고리즘(예: k-평균)을 사용하여 가까운 벡터를 각 하위 공간에서 그룹화합니다. 양자화는 차원 축소와는 다릅니다. 차원 축소에서는 차원 수가 줄어들며, 벡터는 단순히 더 짧아집니다.

#### 계층적 탐색 가능한 작은 세계(HNSW)

이름만 봐도 복잡해 보일 수 있지만 걱정하지 마세요, 실제로 그렇지 않습니다! 간단히 말해, 계층적 탐색 가능한 작은 세계는 매우 인기가 많고 효율적인 다층 그래프 기반 알고리즘입니다. Apache Lucene을 포함한 여러 벡터 데이터베이스에서 사용됩니다. HNSW의 개념적 표현은 아래 그림 12에 나와 있습니다.

**그림 12: 계층적 탐색 가능한 작은 세계**

계층적 탐색 가능한 작은 세계의 시각적 표현
상위 레이어에서는 가장 긴 연결을 가진 매우 적은 수의 벡터 그래프를 볼 수 있습니다. 즉, 유사성이 가장 적은 벡터들로 구성된 연결된 그래프입니다. 더 낮은 레이어로 내려갈수록 더 많은 벡터가 발견되고, 그래프는 점점 더 밀집되어 더 많은 벡터들이 서로 가까워집니다. 가장 낮은 레이어에서는 모든 벡터를 찾을 수 있으며, 가장 유사한 것들이 서로 가장 가까이에 위치합니다.

검색 시점에 알고리즘은 임의의 진입점에서 상위 레이어부터 시작하여 쿼리 벡터(회색 점으로 표시됨)와 가장 가까운 벡터를 찾습니다. 그런 다음, 한 레이어 아래로 이동하여 같은 과정을 반복하며, 위의 레이어에서 남은 동일한 벡터부터 시작합니다. 이렇게 한 레이어씩 내려가면서 가장 낮은 레이어에 도달하여 쿼리 벡터와 가장 가까운 이웃을 찾습니다.

#### 지역 민감 해싱(LSH)

지금까지 소개된 모든 접근 방식과 마찬가지로, 지역 민감 해싱도 검색 공간을 급격히 줄여 검색 속도를 높이는 것을 목표로 합니다. 이 기술을 사용하면 임베딩 벡터가 유사성 정보를 유지하면서 해시 값으로 변환되므로, 최종적으로 검색 공간이 그래프나 트리를 탐색하는 대신 간단한 해시 테이블이 되어 검색 속도를 크게 높일 수 있습니다. 해시 기반 방법의 주요 장점은 임의의(큰) 차원을 가진 벡터를 고정 크기 해시로 매핑할 수 있어 검색 시간을 엄청나게 단축할 수 있다는 점입니다.

일반적으로 데이터를 해싱하는 다양한 방법이 있으며, 특히 임베딩 벡터를 해싱하는 방법도 많지만, 이 기사에서는 각 방법의 세부 사항을 다루지는 않겠습니다. 기존 해싱 방법은 보통 매우 유사해 보이는 데이터에 대해 매우 다른 해시를 생성합니다. 임베딩 벡터는 부동 소수점 값으로 구성되므로, 벡터 산술에서 매우 가까운 것으로 간주되는 두 개의 샘플 부동 소수점 값(예: 0.73과 0.74)을 몇 가지 일반적인 해싱 함수로 처리해봅시다. 아래 결과를 보면, 일반적인 해싱 함수가 입력 간의 유사성을 유지하지 못한다는 것이 명백합니다.

| 해싱 함수 | 0.73                                                             | 0.74                                                             |
| --------- | ---------------------------------------------------------------- | ---------------------------------------------------------------- |
| MD5       | 1342129d04cd2924dd06cead4cf0a3ca                                 | 0aec1b15371bd979cfa66b0a50ebecc5                                 |
| SHA1      | 49d2c3e0e44bff838e1db571a121be5ea874e8d9                         | a534e76482ade9d9fe4bff3035a7f31f2f363d77                         |
| SHA256    | 99d03fc3771fe6848d675339fc49eeb1cb8d99a12e6358173336b99a2ec530ea | 5ecbc825ba5c16856edfdaf0abc5c6c41d0d8a9c508e34188239521dc7645663 |

### 결론을 내리자

벡터 검색을 소개하기 위해, 우리는 이 기사에서 많은 내용을 다루었습니다. 어휘 검색과 벡터 검색의 차이점을 비교한 후, 딥러닝 신경망 모델이 비정형 데이터의 의미를 포착하고, 그 의미를 모델의 각 차원에 따라 데이터의 유사성을 나타내는 부동 소수점 숫자 시퀀스인 고차원 임베딩 벡터로 변환하는 방법을 배웠습니다. 벡터 검색과 어휘 검색은 경쟁하는 정보 검색 기술이 아니라 상호 보완적인 기술이라는 점도 주목할 만합니다(하이브리드 검색을 다룰 세 번째 및 다섯 번째 부분에서 이를 다룰 예정입니다).

그 후, 두 벡터 간의 근접성을 측정하고, 그들이 나타내는 개념의 유사성을 평가할 수 있게 해주는 거리(및 유사성) 함수라는 벡터 검색의 기본 구성 요소를 소개했습니다.

마지막으로, 트리, 그래프, 클러스터 또는 해시를 기반으로 하여, 선형 브루트포스 검색처럼 전체 공간을 탐색하지 않고도 다차원 공간의 특정 영역에 빠르게 집중하여 가장 가까운 이웃을 찾을 수 있는 가장 인기 있는 벡터 검색 알고리즘과 기술의 다양한 버전을 검토했습니다.

---
